{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83706ea3",
   "metadata": {},
   "source": [
    "### analysis pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bedc35ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m┌\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m┐\u001b[0m\u001b[90m\n",
      "\u001b[0m\u001b[90m│\u001b[0m\u001b[90m \u001b[0mcount_star()\u001b[90m \u001b[0m\u001b[90m│\u001b[0m\u001b[90m\n",
      "\u001b[0m\u001b[90m│\u001b[0m\u001b[90m    \u001b[0mint64\u001b[90m     \u001b[0m\u001b[90m│\u001b[0m\u001b[90m\n",
      "\u001b[0m\u001b[90m├\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m┤\u001b[0m\u001b[90m\n",
      "\u001b[0m\u001b[90m│\u001b[0m\u001b[90m     \u001b[0m2709\u001b[90m     \u001b[0m\u001b[90m│\u001b[0m\u001b[90m\n",
      "\u001b[0m\u001b[90m└\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m─\u001b[0m\u001b[90m┘\u001b[0m\u001b[90m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!duckdb scalable.db \".tables\"\n",
    "!duckdb scalable.db \"SELECT COUNT(*) FROM pg_proc;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46e60a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Audit:\n",
      "- Bronze: 4\n",
      "- Silver: 4\n",
      "- Gold:   2\n",
      "\n",
      "BI files ready in: data/outputs/gold_exports/\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import shutil\n",
    "import signal\n",
    "import sys\n",
    "import hashlib\n",
    "\n",
    "con = None\n",
    "\n",
    "def get_file_hash(filepath):\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        while chunk := f.read(65536):\n",
    "            hasher.update(chunk)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    global con\n",
    "    if con: con.close()\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "def run_pipeline(input_dir, db_path=\"../data/outputs/scalable.db\", bronze_dir=\"../data/outputs/bronze_listens\", gold_dir=\"data/outputs/gold_exports\", reset=False):\n",
    "    global con\n",
    "    if reset:\n",
    "        for d in [bronze_dir, gold_dir]:\n",
    "            if os.path.exists(d): shutil.rmtree(d)\n",
    "        if os.path.exists(db_path): os.remove(db_path)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    os.makedirs(bronze_dir, exist_ok=True)\n",
    "    os.makedirs(gold_dir, exist_ok=True)\n",
    "    con = duckdb.connect(db_path)\n",
    "\n",
    "    try:\n",
    "        con.execute(\"SET memory_limit = '4GB'; SET threads = 4; SET preserve_insertion_order = false;\")\n",
    "\n",
    "        con.execute(\"CREATE TABLE IF NOT EXISTS processed_files (content_hash VARCHAR PRIMARY KEY, filename VARCHAR, processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)\")\n",
    "        processed_hashes = set(row[0] for row in con.execute(\"SELECT content_hash FROM processed_files\").fetchall())\n",
    "        \n",
    "        all_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.txt')]\n",
    "        for file_name in all_files:\n",
    "            full_path = os.path.join(input_dir, file_name)\n",
    "            file_hash = get_file_hash(full_path)\n",
    "            if file_hash in processed_hashes: continue \n",
    "\n",
    "            con.execute(f\"\"\"\n",
    "                COPY (SELECT *, (track_metadata->>'track_name') as track_name, (track_metadata->>'artist_name') as artist_name\n",
    "                FROM read_json('{full_path}', format='newline_delimited',\n",
    "                columns={{'user_name': 'VARCHAR', 'listened_at': 'BIGINT', 'recording_msid': 'VARCHAR', 'track_metadata': 'JSON'}}))\n",
    "                TO '{bronze_dir}' (FORMAT 'PARQUET', PARTITION_BY (user_name), OVERWRITE_OR_IGNORE 1)\n",
    "            \"\"\")\n",
    "            con.execute(\"INSERT INTO processed_files (content_hash, filename) VALUES (?, ?)\", [file_hash, file_name])\n",
    "\n",
    "        con.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS silver_listens (\n",
    "                user_name VARCHAR, listened_at BIGINT, recording_msid VARCHAR,\n",
    "                artist_name VARCHAR, track_name VARCHAR, listened_date DATE,\n",
    "                UNIQUE(user_name, listened_at)\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        if any(os.scandir(bronze_dir)):\n",
    "            con.execute(f\"\"\"\n",
    "                INSERT INTO silver_listens (user_name, listened_at, recording_msid, artist_name, track_name, listened_date)\n",
    "                SELECT user_name, listened_at, recording_msid, artist_name, track_name, to_timestamp(listened_at)::DATE\n",
    "                FROM read_parquet('{bronze_dir}/**/*.parquet') ON CONFLICT (user_name, listened_at) DO NOTHING\n",
    "            \"\"\")\n",
    "\n",
    "        con.execute(\"CREATE OR REPLACE VIEW gold_top_10 AS SELECT user_name, COUNT(*) as cnt FROM silver_listens GROUP BY 1 ORDER BY 2 DESC LIMIT 10\")\n",
    "        con.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW gold_user_top_days AS\n",
    "            WITH daily AS (SELECT user_name, listened_date, COUNT(*) as cnt FROM silver_listens GROUP BY 1, 2)\n",
    "            SELECT user_name as user, cnt as number_of_listens, listened_date as date\n",
    "            FROM (SELECT *, ROW_NUMBER() OVER(PARTITION BY user_name ORDER BY cnt DESC) as r FROM daily)\n",
    "            WHERE r <= 3 ORDER BY user ASC, number_of_listens DESC\n",
    "        \"\"\")\n",
    "        con.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW gold_active_user_trends AS\n",
    "            WITH daily_u AS (SELECT listened_date as d, user_name as u FROM silver_listens GROUP BY 1, 2),\n",
    "                 all_d AS (SELECT DISTINCT listened_date as d FROM silver_listens),\n",
    "                 total AS (SELECT COUNT(DISTINCT user_name) as total_cnt FROM silver_listens)\n",
    "            SELECT ad.d as date, COUNT(DISTINCT du.u) as number_active_users,\n",
    "                   ROUND(COUNT(DISTINCT du.u) * 100.0 / (SELECT total_cnt FROM total), 2) as percentage_active_users\n",
    "            FROM all_d ad JOIN daily_u du ON du.d BETWEEN ad.d - INTERVAL 6 DAY AND ad.d GROUP BY 1 ORDER BY 1\n",
    "        \"\"\")\n",
    "\n",
    "        con.execute(f\"COPY gold_user_top_days TO '{gold_dir}/user_peaks.parquet' (FORMAT 'PARQUET')\")\n",
    "        con.execute(f\"COPY gold_active_user_trends TO '{gold_dir}/active_user_trends.parquet' (FORMAT 'PARQUET')\")\n",
    "\n",
    "        b_cnt = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{bronze_dir}/**/*.parquet')\").fetchone()[0]\n",
    "        s_cnt = con.execute(\"SELECT COUNT(*) FROM silver_listens\").fetchone()[0]\n",
    "        g_cnt = con.execute(\"SELECT COUNT(*) FROM gold_user_top_days\").fetchone()[0]\n",
    "        \n",
    "        print(f\"\\nAudit:\\n- Bronze: {b_cnt}\\n- Silver: {s_cnt}\\n- Gold:   {g_cnt}\")\n",
    "        print(f\"\\nBI files ready in: {gold_dir}/\")\n",
    "\n",
    "    finally:\n",
    "        if con: con.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline(\"../data/inputs\", reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bb8ce66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Audit:\n",
      "- Bronze: 4\n",
      "- Silver: 4\n",
      "- Gold:   2\n",
      "\n",
      "BI files ready in: data/outputs/gold_exports/\n",
      "\n",
      "============================================================\n",
      "TASK #2 QUERIES - Using duckdb.query()\n",
      "============================================================\n",
      "\n",
      "A1. Top 10 users by songs listened to:\n",
      "┌───────────┬──────────────┐\n",
      "│ user_name │ listen_count │\n",
      "│  varchar  │    int64     │\n",
      "├───────────┼──────────────┤\n",
      "│ NichoBI   │            4 │\n",
      "└───────────┴──────────────┘\n",
      "\n",
      "\n",
      "A2. Users on 2019-03-01:\n",
      "┌────────────┐\n",
      "│ user_count │\n",
      "│   int64    │\n",
      "├────────────┤\n",
      "│          0 │\n",
      "└────────────┘\n",
      "\n",
      "\n",
      "A3. First song listened to by each user:\n",
      "┌───────────┬──────────────────────────┬─────────────┬───────────────┐\n",
      "│ user_name │        track_name        │ artist_name │ listened_date │\n",
      "│  varchar  │         varchar          │   varchar   │     date      │\n",
      "├───────────┼──────────────────────────┼─────────────┼───────────────┤\n",
      "│ NichoBI   │ Boots of Spanish Leather │ Bob Dylan   │ 2019-04-14    │\n",
      "└───────────┴──────────────────────────┴─────────────┴───────────────┘\n",
      "\n",
      "\n",
      "A4. Daily Active Users (7-day window):\n",
      "┌────────────┬─────────────────────┬─────────────────────────┐\n",
      "│    date    │ number_active_users │ percentage_active_users │\n",
      "│    date    │        int64        │          float          │\n",
      "├────────────┼─────────────────────┼─────────────────────────┤\n",
      "│ 2019-04-14 │                   1 │                   100.0 │\n",
      "│ 2019-04-15 │                   1 │                   100.0 │\n",
      "└────────────┴─────────────────────┴─────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Ensure paths are consistent\n",
    "    INPUT_PATH = \"../data/inputs\"\n",
    "    DB_PATH = \"../data/outputs/scalable.db\"\n",
    "\n",
    "    # 1. Run pipeline (Bronze -> Silver -> Gold)\n",
    "    run_pipeline(INPUT_PATH, db_path=DB_PATH, reset=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TASK #2 QUERIES - Using duckdb.query()\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 2. Connect to the actual file created by run_pipeline\n",
    "    db = duckdb.connect(DB_PATH, read_only=True)\n",
    "    \n",
    "    # A1: Top 10 users\n",
    "    print(\"\\nA1. Top 10 users by songs listened to:\")\n",
    "    # Use duckdb.query(sql, connection=db) as requested\n",
    "    duckdb.query(\"SELECT user_name, COUNT(*) AS listen_count FROM silver_listens GROUP BY 1 ORDER BY 2 DESC LIMIT 10\", connection=db).show()\n",
    "    \n",
    "    # A2: Users on March 1st, 2019\n",
    "    print(\"\\nA2. Users on 2019-03-01:\")\n",
    "    duckdb.query(\"SELECT COUNT(DISTINCT user_name) as user_count FROM silver_listens WHERE listened_date = '2019-03-01'\", connection=db).show()\n",
    "    \n",
    "    # A3: First song per user\n",
    "    print(\"\\nA3. First song listened to by each user:\")\n",
    "    duckdb.query(\"\"\"\n",
    "        SELECT user_name, track_name, artist_name, listened_date\n",
    "        FROM (SELECT *, ROW_NUMBER() OVER(PARTITION BY user_name ORDER BY listened_at ASC) as rn FROM silver_listens)\n",
    "        WHERE rn = 1 ORDER BY user_name\n",
    "    \"\"\", connection=db).show()\n",
    "\n",
    "    # A4: Daily Active Users (7-day rolling window)\n",
    "    print(\"\\nA4. Daily Active Users (7-day window):\")\n",
    "    # Fixed syntax: '6 DAY' instead of '6 DAYS'\n",
    "    duckdb.query(\"\"\"\n",
    "        WITH daily_users AS (\n",
    "            SELECT listened_date, user_name FROM silver_listens GROUP BY 1, 2\n",
    "        ),\n",
    "        all_dates AS (\n",
    "            SELECT DISTINCT listened_date FROM silver_listens\n",
    "        ),\n",
    "        active_counts AS (\n",
    "            SELECT \n",
    "                curr.listened_date,\n",
    "                COUNT(DISTINCT past.user_name) as number_active_users\n",
    "            FROM all_dates curr\n",
    "            LEFT JOIN daily_users past \n",
    "            ON past.listened_date BETWEEN (curr.listened_date - INTERVAL 6 DAY) AND curr.listened_date\n",
    "            GROUP BY curr.listened_date\n",
    "        ),\n",
    "        total_users AS (\n",
    "            SELECT COUNT(DISTINCT user_name) as total FROM silver_listens\n",
    "        )\n",
    "        SELECT \n",
    "            listened_date as date,\n",
    "            number_active_users,\n",
    "            round((number_active_users::FLOAT / (SELECT total FROM total_users)) * 100, 2) as percentage_active_users\n",
    "        FROM active_counts\n",
    "        ORDER BY date ASC\n",
    "    \"\"\", connection=db).show()\n",
    "    \n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b41a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
