{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83706ea3",
   "metadata": {},
   "source": [
    "### analysis pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e3f98",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Resetting environment...\n",
      "-> Gold: Refreshing Views and Exports...\n",
      "------------------------------\n",
      "Audit Result: 0 total records.\n",
      "Valid 2019+ Dates: 0\n",
      "Gold Export ready at: ../data/outputs/gold_exports/user_peaks.parquet\n",
      "------------------------------\n",
      "\n",
      "============================================================\n",
      "TASK #2 QUERIES - Using duckdb.query()\n",
      "============================================================\n",
      "\n",
      "A1. Top 10 users by songs listened to:\n",
      "┌───────────┬──────────────┐\n",
      "│ user_name │ listen_count │\n",
      "│  varchar  │    int64     │\n",
      "├───────────┴──────────────┤\n",
      "│          0 rows          │\n",
      "└──────────────────────────┘\n",
      "\n",
      "\n",
      "A2. Users on 2019-03-01:\n",
      "┌────────────┐\n",
      "│ user_count │\n",
      "│   int64    │\n",
      "├────────────┤\n",
      "│          0 │\n",
      "└────────────┘\n",
      "\n",
      "\n",
      "A3. First song listened to by each user:\n",
      "┌───────────┬────────────┬─────────────┬───────────────┐\n",
      "│ user_name │ track_name │ artist_name │ listened_date │\n",
      "│  varchar  │  varchar   │   varchar   │     date      │\n",
      "├───────────┴────────────┴─────────────┴───────────────┤\n",
      "│                        0 rows                        │\n",
      "└──────────────────────────────────────────────────────┘\n",
      "\n",
      "\n",
      "A4. Daily Active Users (7-day window):\n",
      "┌──────┬─────────────────────┬─────────────────────────┐\n",
      "│ date │ number_active_users │ percentage_active_users │\n",
      "│ date │        int64        │          float          │\n",
      "├──────┴─────────────────────┴─────────────────────────┤\n",
      "│                        0 rows                        │\n",
      "└──────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import shutil\n",
    "import signal\n",
    "import sys\n",
    "import hashlib\n",
    "\n",
    "\n",
    "con = None\n",
    "\n",
    "def get_file_hash(filepath):\n",
    "    \"\"\"Generates a SHA-256 hash of the ENTIRE file for data integrity.\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        while chunk := f.read(65536):\n",
    "            hasher.update(chunk)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    global con\n",
    "    print(\"\\n[!] Interrupt received. Closing DuckDB safely...\")\n",
    "    if con: con.close()\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "def run_pipeline(input_dir, db_path=\"scalable.db\", bronze_dir=\"../data/outputs/bronze_listens\", gold_dir=\"../data/outputs/gold_exports\", reset=False):\n",
    "    global con\n",
    "    if reset:\n",
    "        print(\"[!] Resetting environment...\")\n",
    "        for d in [bronze_dir, gold_dir]:\n",
    "            if os.path.exists(d): shutil.rmtree(d)\n",
    "        if os.path.exists(db_path): os.remove(db_path)\n",
    "    \n",
    "    os.makedirs(bronze_dir, exist_ok=True)\n",
    "    os.makedirs(gold_dir, exist_ok=True)\n",
    "    con = duckdb.connect(db_path)\n",
    "\n",
    "    try:\n",
    "\n",
    "        con.execute(\"SET memory_limit = '4GB'; SET threads = 4;\")\n",
    "\n",
    "\n",
    "        con.execute(\"CREATE TABLE IF NOT EXISTS processed_files (content_hash VARCHAR PRIMARY KEY, filename VARCHAR, processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)\")\n",
    "        processed_hashes = set(row[0] for row in con.execute(\"SELECT content_hash FROM processed_files\").fetchall())\n",
    "        \n",
    "        all_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.txt')]\n",
    "        for file_name in all_files:\n",
    "            full_path = os.path.join(input_dir, file_name)\n",
    "            file_hash = get_file_hash(full_path)\n",
    "            if file_hash in processed_hashes: continue \n",
    "\n",
    "            print(f\"-> Bronze: Ingesting {file_name}\")\n",
    "            con.execute(f\"\"\"\n",
    "                COPY (SELECT *, (track_metadata->>'track_name') as track_name, (track_metadata->>'artist_name') as artist_name\n",
    "                FROM read_json('{full_path}', format='newline_delimited',\n",
    "                columns={{'user_name': 'VARCHAR', 'listened_at': 'BIGINT', 'recording_msid': 'VARCHAR', 'track_metadata': 'JSON'}}))\n",
    "                TO '{bronze_dir}' (FORMAT 'PARQUET', PARTITION_BY (user_name), OVERWRITE_OR_IGNORE 1)\n",
    "            \"\"\")\n",
    "            con.execute(\"INSERT INTO processed_files (content_hash, filename) VALUES (?, ?)\", [file_hash, file_name])\n",
    "\n",
    "        con.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS silver_listens (\n",
    "                user_name VARCHAR, listened_at BIGINT, recording_msid VARCHAR,\n",
    "                artist_name VARCHAR, track_name VARCHAR, listened_date DATE,\n",
    "                UNIQUE(user_name, listened_at)\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        if any(os.scandir(bronze_dir)):\n",
    "            print(\"-> Silver: Syncing and Deduplicating (Seconds Scale)...\")\n",
    "            con.execute(f\"\"\"\n",
    "                INSERT INTO silver_listens (user_name, listened_at, recording_msid, artist_name, track_name, listened_date)\n",
    "                SELECT user_name, listened_at, recording_msid, artist_name, track_name, to_timestamp(listened_at)::DATE\n",
    "                FROM read_parquet('{bronze_dir}/**/*.parquet') ON CONFLICT (user_name, listened_at) DO NOTHING\n",
    "            \"\"\")\n",
    "\n",
    "        # --- JOB 3: GOLD (Analysis Views) ---\n",
    "        print(\"-> Gold: Refreshing Views and Exports...\")\n",
    "        \n",
    "        # View 1: Top 10 Users\n",
    "        con.execute(\"CREATE OR REPLACE VIEW gold_top_10 AS SELECT user_name, COUNT(*) as cnt FROM silver_listens GROUP BY 1 ORDER BY 2 DESC LIMIT 10\")\n",
    "        \n",
    "        # View 2: Top 3 Days per User\n",
    "        con.execute(\"\"\"\n",
    "            CREATE OR REPLACE VIEW gold_user_top_days AS\n",
    "            WITH daily AS (SELECT user_name, listened_date, COUNT(*) as cnt FROM silver_listens GROUP BY 1, 2)\n",
    "            SELECT user_name as user, cnt as number_of_listens, listened_date as date\n",
    "            FROM (SELECT *, ROW_NUMBER() OVER(PARTITION BY user_name ORDER BY cnt DESC) as r FROM daily)\n",
    "            WHERE r <= 3 ORDER BY user ASC, number_of_listens DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        # Physical Parquet Export for BI\n",
    "        con.execute(f\"COPY gold_user_top_days TO '{gold_dir}/user_peaks.parquet' (FORMAT 'PARQUET')\")\n",
    "\n",
    "        # --- AUDIT ---\n",
    "        s_cnt = con.execute(\"SELECT COUNT(*) FROM silver_listens\").fetchone()[0]\n",
    "        v_date = con.execute(\"SELECT COUNT(*) FROM silver_listens WHERE listened_date >= '2019-01-01'\").fetchone()[0]\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Audit Result: {s_cnt} total records.\")\n",
    "        print(f\"Valid 2019+ Dates: {v_date}\")\n",
    "        print(f\"Gold Export ready at: {gold_dir}/user_peaks.parquet\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    finally:\n",
    "        if con: con.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline(\"../data\", reset=True)\n",
    "    \n",
    "    # CORRECT WAY: Use duckdb.connect() to open the database, then query\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TASK #2 QUERIES - Using duckdb.query()\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Open a connection to the database\n",
    "    db = duckdb.connect(\"scalable.db\", read_only=True)\n",
    "    \n",
    "    # A1: Top 10 users\n",
    "    print(\"\\nA1. Top 10 users by songs listened to:\")\n",
    "    duckdb.query(\"SELECT user_name, COUNT(*) AS listen_count FROM silver_listens GROUP BY 1 ORDER BY 2 DESC LIMIT 10\", connection=db).show()\n",
    "    \n",
    "    # A2: Users on March 1st, 2019\n",
    "    print(\"\\nA2. Users on 2019-03-01:\")\n",
    "    duckdb.query(\"SELECT COUNT(DISTINCT user_name) as user_count FROM silver_listens WHERE listened_date = '2019-03-01'\", connection=db).show()\n",
    "    \n",
    "    # A3: First song per user\n",
    "    print(\"\\nA3. First song listened to by each user:\")\n",
    "    duckdb.query(\"\"\"\n",
    "        SELECT user_name, track_name, artist_name, listened_date\n",
    "        FROM (SELECT *, ROW_NUMBER() OVER(PARTITION BY user_name ORDER BY listened_at ASC) as rn FROM silver_listens)\n",
    "        WHERE rn = 1 ORDER BY user_name\n",
    "    \"\"\", connection=db).show()\n",
    "\n",
    "    # A4: Daily Active Users (7-day rolling window)\n",
    "    print(\"\\nA4. Daily Active Users (7-day window):\")\n",
    "    duckdb.query(\"\"\"\n",
    "        WITH daily_users AS (\n",
    "            SELECT listened_date, user_name \n",
    "            FROM silver_listens \n",
    "            GROUP BY 1, 2\n",
    "        ),\n",
    "        all_dates AS (\n",
    "            SELECT DISTINCT listened_date FROM silver_listens\n",
    "        ),\n",
    "        active_counts AS (\n",
    "            SELECT \n",
    "                curr.listened_date,\n",
    "                COUNT(DISTINCT past.user_name) as number_active_users\n",
    "            FROM all_dates curr\n",
    "            LEFT JOIN daily_users past \n",
    "            ON past.listened_date BETWEEN (curr.listened_date - INTERVAL 6 DAYS) AND curr.listened_date\n",
    "            GROUP BY curr.listened_date\n",
    "        ),\n",
    "        total_users AS (\n",
    "            SELECT COUNT(DISTINCT user_name) as total FROM silver_listens\n",
    "        )\n",
    "        SELECT \n",
    "            listened_date as date,\n",
    "            number_active_users,\n",
    "            round((number_active_users::FLOAT / (SELECT total FROM total_users)) * 100, 2) as percentage_active_users\n",
    "        FROM active_counts\n",
    "        ORDER BY date ASC\n",
    "    \"\"\", connection=db).show()\n",
    "    \n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e4720e-8dec-46ed-ac37-f88af8deca77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
